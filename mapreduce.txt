MapReduce定义
	Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架。
	Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。
	1．MapReduce 易于编程
		它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。
	2．良好的扩展性
		当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。
	3．高容错性
		MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。
	4．适合PB级以上海量数据的离线处理
		这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，MapReduce很难做到。

MapReduce不擅长做实时计算、流式计算、DAG（有向图）计算。
	1.实时计算
		MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。
	2.流式计算
		流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。
	3.DAG（有向图）计算
		多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。
		
图4.1

1）分布式的运算程序往往需要分成至少2个阶段。
2）第一个阶段的maptask并发实例，完全并行运行，互不相干。
3）第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出。
4）MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行。

1.4 MapReduce进程
	一个完整的mapreduce程序在分布式运行时有三类实例进程：
		1）MrAppMaster：负责整个程序的过程调度及状态协调。
		2）MapTask：负责map阶段的整个数据处理流程。
		3）ReduceTask：负责reduce阶段的整个数据处理流程。
	
MapReduce编程规范
用户编写的程序分成三个部分：Mapper、Reducer和Driver。
	1．Mapper阶段（k-long首字母的偏移量，v-string  记录一行的数据）
		（1）用户自定义的Mapper要继承自己的父类==？？？？？=====Mapper<LongWritable, Text, Text, IntWritable>
		（2）Mapper的输入数据是KV对的形式（KV的类型可自定义）
		（3）Mapper中的业务逻辑写在map()方法中
		（4）Mapper的输出数据是KV对的形式（KV的类型可自定义）
		（5）map()方法（maptask进程）对每一个<K,V>调用一次
	2．Reducer阶段
		（1）用户自定义的Reducer要继承自己的父类
		（2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV
		（3）Reducer的业务逻辑写在reduce()方法中
		（4）Reducetask进程对每一组相同k的<k,v>组调用一次reduce()方法
	3．Driver阶段
		相当于yarn集群的客户端，用于提交我们整个程序到yarn集群，提交的是封装了mapreduce程序相关运行参数的job对象
		
		
 * @param <KEYIN> the key input type to the Mapper
 * @param <VALUEIN> the value input type to the Mapper
 * @param <KEYOUT> the key output type from the Mapper
 * @param <VALUEOUT> the value output type from the Mapper
 
 导入配置文件
<dependencies>
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<version>RELEASE</version>
		</dependency>
		<dependency>
			<groupId>org.apache.logging.log4j</groupId>
			<artifactId>log4j-core</artifactId>
			<version>2.8.2</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-common</artifactId>
			<version>2.7.2</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-client</artifactId>
			<version>2.7.2</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-hdfs</artifactId>
			<version>2.7.2</version>
		</dependency>
</dependencies>

在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入file文件
log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n

（1）编写mapper类
			package com.atguigu.mapreduce;
			import java.io.IOException;
			import org.apache.hadoop.io.IntWritable;
			import org.apache.hadoop.io.LongWritable;
			import org.apache.hadoop.io.Text;
			import org.apache.hadoop.mapreduce.Mapper;

			public class WordcountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{//mapper 类 就代表整个mapper 过程
																					//mapper方法参数 LongWritable worker 传入的key类型，一行的偏移量
																					Text  是value 的类型默认一行文本类容
																					Text 产生结果的数据类型key ==mapper输出
																					IntWritable 结果数据的value 类型   ==mapper输出
				Text k = new Text();
				IntWritable v = new IntWritable(1); //单词一个，输出时需要转为int类型，每个单词都是一个
				
				@Override
				protected void map(LongWritable key, Text value, Context context)//这里面的参数表示key是每个单词，value 是int，与mapper类的参数KEY,VALUE不一样
						throws IOException, InterruptedException {
					//偏移量：	
								0=============MT202-Settle(EXCL)1context=====org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context@61277af0
								21=============MT202-Settle(EXCL)2context
								从0开始，MT202-Settle(EXCL)1  19个字符数，加上回车20个，下一行从21开始，实际回车一般占两个字符
					// 1 获取一行
					String line = value.toString();
					
					// 2 切割
					String[] words = line.split(" ");
					
					// 3 输出
					for (String word : words) {
						
						k.set(word);//拆分出来的单词作为key,v 代表单词单词个数1
						context.write(k, v);
					}

				}
			}


（2）编写reducer类
package com.atguigu.mapreduce.wordcount;
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class WordcountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{//就是接受 mapper 输出的，key,value 与mapper的后两个参数类型 一致，代表整个reduce，并输出

int sum;
IntWritable v = new IntWritable();

	@Override
	protected void reduce(Text key, Iterable<IntWritable> value,
			Context context) throws IOException, InterruptedException {
		
		// 1 累加求和
		sum = 0;
		for (IntWritable count : value) {
			sum += count.get();
		}
		
		// 2 输出
        v.set(sum);
		context.write(key,v);
	}
}
（3）编写驱动类
package com.atguigu.mapreduce.wordcount;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordcountDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

		// 1 获取配置信息以及封装任务
		Configuration configuration = new Configuration();
		Job job = Job.getInstance(configuration);

		// 2 设置jar加载路径
		job.setJarByClass(WordcountDriver.class);

		// 3 设置map和reduce类
		job.setMapperClass(WordcountMapper.class);
		job.setReducerClass(WordcountReducer.class);

		// 4 设置map输出
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);

		// 5 设置Reduce输出
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		// 6 设置输入和输出路径
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		// 7 提交
		boolean result = job.waitForCompletion(true);

		System.exit(result ? 0 : 1);
	}
}

idea 运行时：
	programe arguments添加：C:\bigbigbig\input C:\bigbigbig\output
	保证C:\bigbigbig\路径下面没有output 程序会自动生成



什么是序列化
	序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。 
	反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。
	
2.1.2 为什么要序列化
    一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。

2.1.3 为什么不用Java的序列化
	 Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），特点如下：
	1．紧凑 
	紧凑的格式能让我们充分利用网络带宽，而带宽是数据中心最稀缺的资源
	2．快速
	进程通信形成了分布式系统的骨架，所以需要尽量减少序列化和反序列化的性能开销，这是基本的；
	3．可扩展
	协议为了满足新的需求变化，所以控制客户端和服务器过程中，需要直接引进相应的协议，这些是新协议，原序列化方式能支持新的协议报文；
	4．互操作
	能支持不同语言写的客户端和服务端进行交互； 
	
2.2 常用数据序列化类型
	表4-1 常用的数据类型对应的hadoop数据序列化类型
	Java类型	Hadoop Writable类型
	boolean	BooleanWritable
	byte	ByteWritable
	int	IntWritable
	float	FloatWritable
	long	LongWritable
	double	DoubleWritable
	String	Text
	map	MapWritable
	
自定义bean对象实现序列化接口（Writable）
	自定义bean对象要想序列化传输，必须实现序列化接口，需要注意以下7项。
	（1）必须实现Writable接口
	（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造
	public FlowBean() {
		super();
	}
（3）重写序列化方法
	@Override
		public void write(DataOutput out) throws IOException {
			out.writeLong(upFlow);
			out.writeLong(downFlow);
			out.writeLong(sumFlow);
		}
（4）重写反序列化方法
	@Override
	public void readFields(DataInput in) throws IOException {
		upFlow = in.readLong();
		downFlow = in.readLong();
		sumFlow = in.readLong();
	}
（5）注意反序列化的顺序和序列化的顺序完全一致
（6）要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用。
（7）如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序。
	@Override
	public int compareTo(FlowBean o) {
		// 倒序排列，从大到小
		return this.sumFlow > o.getSumFlow() ? -1 : 1;
	}
	
基本思路：
Map阶段：
	（1）读取一行数据，切分字段
	（2）抽取手机号、上行流量、下行流量
	（3）以手机号为key，bean对象为value输出，即context.write(手机号,bean);

Reduce阶段：
	（1）累加上行流量和下行流量得到总流量。
	（2）实现自定义的bean来封装流量信息，并将bean作为map输出的key来传输
	（3）MR程序在处理数据的过程中会对数据排序(map输出的kv对传输到reduce之前，会排序)，排序的依据是map输出的key
	所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到key中，让key实现接口：WritableComparable。
	然后重写key的compareTo方法。
	
	package com.atguigu.mapreduce.flowsum;
	import java.io.DataInput;
	import java.io.DataOutput;
	import java.io.IOException;
	import org.apache.hadoop.io.Writable;

	// 1 实现writable接口
	public class FlowBean implements Writable{

		private long upFlow ;//首先需要封装的东西，根据需求
		private long downFlow;
		private long sumFlow;
		
		//2  反序列化时，需要反射调用空参构造函数，所以必须有
		public FlowBean() {
			super();
		}

		public FlowBean(long upFlow, long downFlow) {
			super();
			this.upFlow = upFlow;
			this.downFlow = downFlow;
			this.sumFlow = upFlow + downFlow;
		}
	
		//3  写序列化方法
		@Override
		public void write(DataOutput out) throws IOException {
			out.writeLong(upFlow);
			out.writeLong(downFlow);
			out.writeLong(sumFlow);
		}
	
		//4 反序列化方法
		//5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致
		这不能定义变量  要赋值给属性
		@Override
			public void readFields(DataInput in) throws IOException {
				this.upFlow  = in.readLong();
				this.downFlow = in.readLong();
				this.sumFlow = in.readLong();
			}

			// 6 编写toString方法，方便后续打印到文本
			@Override
			public String toString() {
				return upFlow + "\t" + downFlow + "\t" + sumFlow;
			}

			public long getUpFlow() {
				return upFlow;
			}

			public void setUpFlow(long upFlow) {
				this.upFlow = upFlow;
			}

			public long getDownFlow() {
				return downFlow;
			}
			

package com.atguigu.mapreduce.flowsum;
import java.io.IOException;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class FlowCountMapper extends Mapper<LongWritable, Text, Text, FlowBean>{
	
	FlowBean v = new FlowBean();
	Text k = new Text();
	
	@Override
	protected void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {
		
		// 1 获取一行
		String line = value.toString();
		
		// 2 切割字段
		String[] fields = line.split("\t");
		
		// 3 封装对象
		// 取出手机号码
		String phoneNum = fields[1];
		// 取出上行流量和下行流量
		long upFlow = Long.parseLong(fields[fields.length - 3]);
		long downFlow = Long.parseLong(fields[fields.length - 2]);
		
		v.set(downFlow, upFlow);
		
		// 4 写出
		context.write(new Text(phoneNum), new FlowBean(upFlow, downFlow));
	}
}

package com.atguigu.mapreduce.flowsum;
import java.io.IOException;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class FlowCountReducer extends Reducer<Text, FlowBean, Text, FlowBean> {

	@Override
	protected void reduce(Text key, Iterable<FlowBean> values, Context context)
			throws IOException, InterruptedException {

		long sum_upFlow = 0;
		long sum_downFlow = 0;

		// 1 遍历所用bean，将其中的上行流量，下行流量分别累加
		for (FlowBean flowBean : values) {
			sum_upFlow += flowBean.getSumFlow();
			sum_downFlow += flowBean.getDownFlow();
		}

		// 2 封装对象
		FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow);
		
		// 3 写出
		context.write(key, resultBean);
	}
}

package com.atguigu.mapreduce.flowsum;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class FlowsumDriver {

	public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException {
		
		// 1 获取配置信息，或者job对象实例
		Configuration configuration = new Configuration();
		Job job = Job.getInstance(configuration);

		// 6 指定本程序的jar包所在的本地路径
		job.setJarByClass(FlowsumDriver.class);

		// 2 指定本业务job要使用的mapper/Reducer业务类
		job.setMapperClass(FlowCountMapper.class);
		job.setReducerClass(FlowCountReducer.class);

		// 3 指定mapper输出数据的kv类型
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(FlowBean.class);

		// 4 指定最终输出的数据的kv类型
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(FlowBean.class);
		
		// 5 指定job的输入原始文件所在目录
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行
		boolean result = job.waitForCompletion(true);
		System.exit(result ? 0 : 1);
	}
}


mapreduce最全工作流程:
